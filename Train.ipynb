{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d215e447-f153-4454-9e74-0a6ed59bb15c",
   "metadata": {},
   "source": [
    " # Train file \n",
    " ---\n",
    " For the model we will start by showing that we can purely train on depths 1 and 2 as those are the first instances containing every possible block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f27e1-e8ec-4db0-b2a2-d7ffc4b8b776",
   "metadata": {},
   "source": [
    "## Shift in the overall goal of this project:\n",
    "---\n",
    "Originally this project started with the goal to combine two networks together to create stereoscopic images based on surrealist art. However, this process, while definitely possible through sets of linear translations and object degmentation, is impractical for real runtime arguments. So here in the notebooks we provide a proof of concept for the network, however we will be turning this into a script for the Blender and Maxwell tools for 3-D modelling\n",
    "\n",
    "Blender: Construct a 3D set of objects in a constrained 3-D domain\n",
    "Maxwell: Render these objects\n",
    "\n",
    "For now we will focus on a proof of concept ProGAN, and from there we repurpose this gan for our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77657c6-ca7b-4210-89a3-022f21ed9609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\campb\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\campb\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "### - imports - ###\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "### - other data augmentation imports - ### (if needed)\n",
    "### - Imports - ###\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn as sk #general imports, initial data preprocessing/OS stuff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim #Neural network imports, multiply data etc\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn.functional as F #Neural Network used in Comp4660 at ANU\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler #normalize data\n",
    "from sklearn.metrics import confusion_matrix #analysis\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from NetworkMain import D, G\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7179e90-3137-4cf6-9c6b-8e93fe29d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will be added later when the file is converted to a python file\n",
    "\"\"\"parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root', type=str, default='./', help='directory contrains the data and outputs')\n",
    "parser.add_argument('--epochs', type=int, default=40, help='training epoch number')\n",
    "parser.add_argument('--out_res', type=int, default=128, help='The resolution of final output image')\n",
    "parser.add_argument('--resume', type=int, default=0, help='continues from epoch number')\n",
    "parser.add_argument('--cuda', action='store_true', help='Using GPU to train')\n",
    "\n",
    "\n",
    "opt = parser.parse_args()\"\"\"\n",
    "\n",
    "root = '/Users/campb/Documents/PersonalProjects/AGRNet/'\n",
    "data_dir = root + 'Dataset/'\n",
    "check_point_dir = root + 'check_points/'\n",
    "output_dir = root + 'output/'\n",
    "weight_dir = root+ 'weight/'\n",
    "if not os.path.exists(check_point_dir):\n",
    "\tos.makedirs(check_point_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.makedirs(output_dir)\n",
    "if not os.path.exists(weight_dir):\n",
    "\tos.makedirs(weight_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f28971-e8b5-431e-93b8-d7e0dbbc57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Global Variables - ###\n",
    "DFP = '/Users/campb/Documents/PersonalProjects/AGRNet/Dataset/Sample-'\n",
    "\n",
    "img_folder = '/Users/campb/Documents/PersonalProjects/AGRNet/Dataset/'\n",
    "\n",
    "NS = '/Sample-'\n",
    "\n",
    "image_format = 'RGB'\n",
    "\n",
    "schedule = [[5, 15, 25 ,35, 40],[6, 5, 5, 5, 4],[5, 5, 5, 1, 1]] #Epochs for each cycle num of epochs\n",
    "\n",
    "batch_size = schedule[1][0]\n",
    "\n",
    "growing = schedule[2][0]\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "latent_size = 512\n",
    "\n",
    "out_res = 1024\n",
    "\n",
    "lr = 1e-5\n",
    "\n",
    "lambd = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0cca62-5f79-4e2b-88a4-cbe713331853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = T.device('cuda:0' if (T.cuda.is_available())  else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(out_res),\n",
    "            transforms.CenterCrop(out_res),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab319914-ad8f-4e7d-8277-2c86846b1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create networks\n",
    "Disc = None\n",
    "Gen = None\n",
    "Disc = D(latent_size, out_res).to(device)\n",
    "Gen = G(latent_size, out_res).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b42f5d-48e8-4639-9185-8fd1359519ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise for discriminator\n",
    "fixed_noise = T.randn(16, latent_size, 1, 1).to(device)\n",
    "#initialize optimizers\n",
    "D_optimizer = optim.Adam(Disc.parameters(), lr=lr, betas=(0, 0.99))\n",
    "G_optimizer = optim.Adam(Gen.parameters(), lr=lr, betas=(0, 0.99))\n",
    "#Metric variables\n",
    "D_running_loss = 0.0\n",
    "G_running_loss = 0.0\n",
    "iter_num = 0\n",
    "\n",
    "D_epoch_losses = []\n",
    "G_epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31937179-b410-4458-a030-578806d24dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load with ImageFolder wrapper\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,img_folder,names, transform):\n",
    "        self.transform = transform\n",
    "        self.image_names = names #Predetermined dataset\n",
    "        self.img_folder=img_folder\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image=Image.open(self.img_folder+self.image_names[index]).convert(\"RGB\")\n",
    "        image=self.transform(image)\n",
    "        return image\n",
    "\n",
    "assert(os.path.exists(DFP + str(1) + \".jpg\"))\n",
    "rawimgf = sorted(glob.glob(DFP + '*.jpg', recursive = True))\n",
    "### - image names - ###\n",
    "imnames = [i.split('/')[-1].split(\"t\")[-1][1:] for i in rawimgf]\n",
    "\n",
    "### - Global data loader Vars - ###\n",
    "norms = (0.5,0.5,0.5), (0.5,0.5,0.5)\n",
    "train_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((out_res, out_res))])\n",
    "# Parameters\n",
    "params = {'batch_size': schedule[1][0],\n",
    "          'shuffle': True,\n",
    "         'num_workers': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374fa1ea-fc48-4646-83ff-53d980ec488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nexcept:\\n    print('Fully Grown\\n')\\n    c = -1\\n    batch_size = schedule[1][c]\\n    growing = schedule[2][c]\\n\\n    dataset = ImageDataset(img_folder,imnames, train_transform)\\n    data_loader = DataLoader(dataset, **params)\\n\\n    tot_iter_num = (len(dataset)/batch_size)\\n    print(schedule[0][c], resume)\\n\\n    if Gen.alpha < 1:\\n        Gen.fade_iters = (1-Gen.alpha)/(opt.epochs-opt.resume)/(2*tot_iter_num)\\n        Ddisc.fade_iters = (1-Disc.alpha)/(opt.epochs-opt.resume)/(2*tot_iter_num)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Control variable\n",
    "resume = 0\n",
    "if resume != 0:\n",
    "    check_point = T.load(check_point_dir+'check_point_epoch_%i.pth' % resume)\n",
    "    fixed_noise = check_point['fixed_noise']\n",
    "    G_net.load_state_dict(check_point['G_net'])\n",
    "    D_net.load_state_dict(check_point['D_net'])\n",
    "    G_optimizer.load_state_dict(check_point['G_optimizer'])\n",
    "    D_optimizer.load_state_dict(check_point['D_optimizer'])\n",
    "    G_epoch_losses = check_point['G_epoch_losses']\n",
    "    D_epoch_losses = check_point['D_epoch_losses']\n",
    "    G_net.depth = check_point['depth']\n",
    "    D_net.depth = check_point['depth']\n",
    "    G_net.alpha = check_point['alpha']\n",
    "    D_net.alpha = check_point['alpha']\n",
    "\n",
    "\n",
    "\n",
    "c = next(x[0] for x in enumerate(schedule[0]) if x[1]>resume)-1\n",
    "batch_size = schedule[1][c]\n",
    "growing = schedule[2][c]\n",
    "dataset = ImageDataset(img_folder, imnames, train_transform)\n",
    "# dataset = datasets.CelebA(data_dir, split='all', transform=transform)\n",
    "data_loader = DataLoader(dataset, **params)\n",
    "\n",
    "tot_iter_num = (len(dataset)/batch_size)\n",
    "Gen.fade_iters = (1-Gen.alpha)/(schedule[0][c+1]-resume)/(2*tot_iter_num)\n",
    "Disc.fade_iters = (1-Disc.alpha)/(schedule[0][c+1]-resume)/(2*tot_iter_num)\n",
    "\n",
    "\"\"\"\n",
    "except:\n",
    "    print('Fully Grown\\n')\n",
    "    c = -1\n",
    "    batch_size = schedule[1][c]\n",
    "    growing = schedule[2][c]\n",
    "\n",
    "    dataset = ImageDataset(img_folder,imnames, train_transform)\n",
    "    data_loader = DataLoader(dataset, **params)\n",
    "\n",
    "    tot_iter_num = (len(dataset)/batch_size)\n",
    "    print(schedule[0][c], resume)\n",
    "\n",
    "    if Gen.alpha < 1:\n",
    "        Gen.fade_iters = (1-Gen.alpha)/(opt.epochs-opt.resume)/(2*tot_iter_num)\n",
    "        Ddisc.fade_iters = (1-Disc.alpha)/(opt.epochs-opt.resume)/(2*tot_iter_num)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a8ab5-7099-4387-aa96-99c383b799f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Resolution: 4 x 4\n",
      "epoch: 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 2.170   G_loss: 0.030:   4%|█▉                                                  | 1/27 [00:02<01:11,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 2.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 2.210   G_loss: 0.054:  19%|█████████▋                                          | 5/27 [00:03<00:08,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 2.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 2.226   G_loss: 0.048:  30%|███████████████▍                                    | 8/27 [00:04<00:04,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 2.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.800   G_loss: 0.046:  41%|████████████████████▊                              | 11/27 [00:04<00:03,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 9, gp: 2.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.667   G_loss: 0.060:  52%|██████████████████████████▍                        | 14/27 [00:05<00:02,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 12, gp: 1.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.667   G_loss: 0.060:  63%|████████████████████████████████                   | 17/27 [00:05<00:01,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 15, gp: 1.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.174   G_loss: 0.072:  74%|█████████████████████████████████████▊             | 20/27 [00:06<00:01,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 18, gp: 1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.093   G_loss: 0.078:  85%|███████████████████████████████████████████▍       | 23/27 [00:06<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 21, gp: 1.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.015   G_loss: 0.089:  96%|█████████████████████████████████████████████████  | 26/27 [00:07<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 24, gp: 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.015   G_loss: 0.089: 100%|███████████████████████████████████████████████████| 27/27 [00:07<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.921   G_loss: 0.093:   0%|                                                            | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.670   G_loss: 0.099:  15%|███████▋                                            | 4/27 [00:00<00:04,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.650   G_loss: 0.097:  26%|█████████████▍                                      | 7/27 [00:01<00:03,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.442   G_loss: 0.091:  41%|████████████████████▊                              | 11/27 [00:02<00:02,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 9, gp: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.249   G_loss: 0.109:  52%|██████████████████████████▍                        | 14/27 [00:02<00:02,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 12, gp: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.375   G_loss: 0.127:  63%|████████████████████████████████                   | 17/27 [00:03<00:01,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 15, gp: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.187   G_loss: 0.122:  74%|█████████████████████████████████████▊             | 20/27 [00:03<00:01,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 18, gp: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.214   G_loss: 0.139:  85%|███████████████████████████████████████████▍       | 23/27 [00:04<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 21, gp: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.087   G_loss: 0.149:  96%|█████████████████████████████████████████████████  | 26/27 [00:04<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 24, gp: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.087   G_loss: 0.149: 100%|███████████████████████████████████████████████████| 27/27 [00:04<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.317   G_loss: 0.152:   7%|███▊                                                | 2/27 [00:00<00:04,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.378   G_loss: 0.162:  19%|█████████▋                                          | 5/27 [00:00<00:04,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.301   G_loss: 0.166:  30%|███████████████▍                                    | 8/27 [00:01<00:03,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.479   G_loss: 0.166:  41%|████████████████████▊                              | 11/27 [00:01<00:02,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 9, gp: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.660   G_loss: 0.184:  52%|██████████████████████████▍                        | 14/27 [00:02<00:02,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 12, gp: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.615   G_loss: 0.174:  63%|████████████████████████████████                   | 17/27 [00:03<00:01,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 15, gp: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.695   G_loss: 0.217:  74%|█████████████████████████████████████▊             | 20/27 [00:03<00:01,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 18, gp: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.786   G_loss: 0.189:  85%|███████████████████████████████████████████▍       | 23/27 [00:04<00:00,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 21, gp: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.724   G_loss: 0.227:  93%|███████████████████████████████████████████████▏   | 25/27 [00:04<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 24, gp: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.724   G_loss: 0.227: 100%|███████████████████████████████████████████████████| 27/27 [00:04<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.000   G_loss: 0.218:   0%|                                                            | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.072   G_loss: 0.239:  19%|█████████▋                                          | 5/27 [00:00<00:03,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.118   G_loss: 0.241:  30%|███████████████▍                                    | 8/27 [00:01<00:03,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.096   G_loss: 0.239:  41%|████████████████████▊                              | 11/27 [00:02<00:02,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 9, gp: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 0.843   G_loss: 0.248:  52%|██████████████████████████▍                        | 14/27 [00:02<00:02,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 12, gp: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.048   G_loss: 0.284:  63%|████████████████████████████████                   | 17/27 [00:03<00:01,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 15, gp: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.266   G_loss: 0.260:  70%|███████████████████████████████████▉               | 19/27 [00:03<00:01,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 18, gp: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.214   G_loss: 0.279:  85%|███████████████████████████████████████████▍       | 23/27 [00:04<00:00,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 21, gp: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.500   G_loss: 0.289:  96%|█████████████████████████████████████████████████  | 26/27 [00:04<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 24, gp: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.500   G_loss: 0.289: 100%|███████████████████████████████████████████████████| 27/27 [00:04<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.111   G_loss: 0.313:   7%|███▊                                                | 2/27 [00:00<00:04,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.455   G_loss: 0.311:  19%|█████████▋                                          | 5/27 [00:00<00:04,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.519   G_loss: 0.295:  26%|█████████████▍                                      | 7/27 [00:01<00:03,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.289   G_loss: 0.328:  41%|████████████████████▊                              | 11/27 [00:02<00:02,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 9, gp: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.344   G_loss: 0.345:  52%|██████████████████████████▍                        | 14/27 [00:02<00:02,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 12, gp: 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.614   G_loss: 0.333:  63%|████████████████████████████████                   | 17/27 [00:03<00:01,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 15, gp: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.499   G_loss: 0.353:  74%|█████████████████████████████████████▊             | 20/27 [00:03<00:01,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 18, gp: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.535   G_loss: 0.344:  85%|███████████████████████████████████████████▍       | 23/27 [00:04<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 21, gp: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.615   G_loss: 0.369:  93%|███████████████████████████████████████████████▏   | 25/27 [00:04<00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 24, gp: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 1.615   G_loss: 0.369: 100%|███████████████████████████████████████████████████| 27/27 [00:04<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing network to size: 8\n",
      "Output Resolution: 8 x 8\n",
      "epoch: 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 2.711   G_loss: 0.289:   4%|█▉                                                  | 1/27 [00:00<00:17,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, gp: 5.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 5.062   G_loss: 0.108:  15%|███████▋                                            | 4/27 [00:01<00:07,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 3, gp: 5.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 4.766   G_loss: 0.121:  26%|█████████████▍                                      | 7/27 [00:02<00:05,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 6, gp: 5.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D_loss: 4.766   G_loss: 0.121:  30%|███████████████▍                                    | 8/27 [00:02<00:05,  3.25it/s]"
     ]
    }
   ],
   "source": [
    "### - Train Loop - ###\n",
    "size = 2**(Gen.depth+1)\n",
    "print(\"Output Resolution: %d x %d\" % (size, size))\n",
    "for epoch in range(1+resume, epochs+1):\n",
    "    Gen.train()\n",
    "    D_epoch_loss = 0.0\n",
    "    G_epoch_loss = 0.0\n",
    "    if epoch-1 in schedule[0]:\n",
    "\n",
    "        if (2 **(Gen.depth +1) < out_res):\n",
    "            c = schedule[0].index(epoch-1)\n",
    "            batch_size = schedule[1][c]\n",
    "            growing = schedule[2][0]\n",
    "            print(\"Growing network to size: \" + str(2**(Gen.depth+2)))\n",
    "            data_loader = DataLoader(dataset, **params)\n",
    "            tot_iter_num = tot_iter_num = (len(dataset)/batch_size)\n",
    "            Gen.inc_depth(growing*tot_iter_num)\n",
    "            Disc.inc_depth(growing*tot_iter_num)\n",
    "            size = 2**(Gen.depth+1)\n",
    "            print(\"Output Resolution: %d x %d\" % (size, size))\n",
    "\n",
    "\t\n",
    "    print(\"epoch: %i/%i\" % (int(epoch), int(epochs)))\n",
    "    databar = tqdm(data_loader)\n",
    "    for i, samples in enumerate(databar):\n",
    "        ##  update D\n",
    "        if size != out_res: #Basically need to, A Reshape, B prepare the data for the networks\n",
    "            samples = F.interpolate(samples, size=(size,size)).to(device)\n",
    "        else:\n",
    "            samples = samples.to(device)\n",
    "        Disc.zero_grad()\n",
    "        noise = T.randn(samples.size(0), latent_size, 1, 1, device=device)\n",
    "        fake = Gen(noise)\n",
    "        #out_grid = make_grid(fake, normalize=True, nrow=4, scale_each=True, padding=int(0.5*(2**Gen.depth))).permute(1,2,0)\n",
    "        #plt.imshow(out_grid.cpu())\n",
    "        fake_out = Disc(fake.detach())\n",
    "        real_out = Disc(samples)\n",
    "        ## Gradient Penalty\n",
    "        \n",
    "        eps = T.rand(samples.size(0), 1, 1, 1, device=device)\n",
    "        eps = eps.expand_as(Gen(noise))\n",
    "        x_hat = eps * samples + (1 - eps) * fake.detach()\n",
    "        x_hat.requires_grad = True\n",
    "        px_hat = Disc(x_hat)\n",
    "        grad = T.autograd.grad(\n",
    "                                    outputs = px_hat.sum(),\n",
    "                                    inputs = x_hat, \n",
    "                                    create_graph=True\n",
    "                                    )[0]\n",
    "        grad_norm = grad.view(samples.size(0), -1).norm(2, dim=1)\n",
    "        gradient_penalty = lambd * ((grad_norm  - 1)**2).mean()\n",
    "        ###########\n",
    "\n",
    "        D_loss = fake_out.mean() - real_out.mean() + gradient_penalty\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        ##\tupdate G\n",
    "\n",
    "        Gen.zero_grad()\n",
    "        fake_out = Disc(fake)\n",
    "\n",
    "        G_loss = - fake_out.mean()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        ##############\n",
    "        D_running_loss += abs(D_loss.item())\n",
    "        G_running_loss += abs(G_loss.item())\n",
    "        iter_num += 1\n",
    "        if i % 3== 0:\n",
    "            D_running_loss /= iter_num\n",
    "            G_running_loss /= iter_num\n",
    "            print('iteration : %d, gp: %.2f' % (i, gradient_penalty))\n",
    "            databar.set_description('D_loss: %.3f   G_loss: %.3f' % (D_running_loss ,G_running_loss))\n",
    "            iter_num = 0\n",
    "            D_running_loss = 0.0\n",
    "            G_running_loss = 0.0\n",
    "\n",
    "\n",
    "    D_epoch_losses.append(D_epoch_loss/tot_iter_num)\n",
    "    G_epoch_losses.append(G_epoch_loss/tot_iter_num)\n",
    "\n",
    "\n",
    "    check_point = {'G_net' : Gen.state_dict(), \n",
    "                   'G_optimizer' : G_optimizer.state_dict(),\n",
    "                   'D_net' : Disc.state_dict(),\n",
    "                   'D_optimizer' : D_optimizer.state_dict(),\n",
    "                   'D_epoch_losses' : D_epoch_losses,\n",
    "                   'G_epoch_losses' : G_epoch_losses,\n",
    "                   'fixed_noise': fixed_noise,\n",
    "                   'depth': Gen.depth,\n",
    "                   'alpha':Gen.alpha\n",
    "                   }\n",
    "    with T.no_grad():\n",
    "        Gen.eval()\n",
    "        T.save(check_point, check_point_dir + 'check_point_epoch_%d.pth' % (epoch))\n",
    "        T.save(Gen.state_dict(), weight_dir + 'G_weight_epoch_%d.pth' %(epoch))\n",
    "        out_imgs = Gen(fixed_noise).to(device)\n",
    "        out_grid = make_grid(out_imgs, normalize=True, nrow=4, scale_each=True, padding=int(0.5*(2**Gen.depth))).permute(1,2,0)\n",
    "        plt.imshow(out_grid.cpu())\n",
    "        plt.savefig(output_dir + 'size_%i_epoch_%d' %(size ,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02407ee-e17b-438e-80a6-32a0a18126d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26dfe1-028f-43d6-8ec6-950001e1f936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745cf4b9-62a9-46db-a619-6909f01f2a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
