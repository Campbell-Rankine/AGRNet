{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c0901a-393d-44d3-984e-5a91c421bd38",
   "metadata": {},
   "source": [
    "# Construct GAN and structure\n",
    "---\n",
    "Construct GAN, define training loop with multithreaded approach, and practice using industry standard terminal initialization commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7506e300-805a-45d3-bad0-681763121f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - IMPORTS - ###\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from multiprocessing.dummy import Pool as TP\n",
    "import cv2 as cv\n",
    "import os\n",
    "import random\n",
    "### - other data augmentation imports - ### (if needed)\n",
    "### - Imports - ###\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim #Neural network imports, multiply data etc\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torch.nn.functional as F #Neural Network used in Comp4660 at ANU\n",
    "\n",
    "from Utils.NetworkHelpers import EqualizedLR_Conv2d, Pixel_norm, Minibatch_std\n",
    "\n",
    "### - Other global variables - ###\n",
    "LOVTV = [15, 26, 66] ##Training values to leave out\n",
    "\n",
    "img_folder = '/Users/campb/Documents/PersonalProjects/AGRNet/Dataset/'\n",
    "\n",
    "NS = '/Sample-'\n",
    "\n",
    "image_format = 'RGB'\n",
    "\n",
    "imsize = 4\n",
    "\n",
    "multiplication_factor = 20\n",
    "\n",
    "num_channels=3\n",
    "kernal=4\n",
    "s=2\n",
    "p=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b0c98949-eda6-42ac-a278-9bb661ca8b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19fabaf3a90>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "T.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f95f409c-5955-47eb-8d6e-e4299d3538dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - Train help functions - ###\n",
    "#Cyclic LR Scheduler\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "class fRGB(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.cvt = EqualizedLR_Conv2d(in_c, out_c, kernal_size=(1,1), stride=(1,1))\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.relu(x)\n",
    "        \n",
    "class tRGB(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.cvt = EqualizedLR_Conv2d(in_c, out_c, kernal_size=(1,1), stride=(1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.cvt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "44aa73ff-d1ff-4c88-8e29-a8891fb8628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator block\n",
    "class D_Cell(nn.Module):\n",
    "    def __init__(self, in_c, out_c, sb=0):\n",
    "        self.sb = sb\n",
    "        super().__init__()\n",
    "        \n",
    "        #Define network structure                                                         #initial block b (1-alpha)\n",
    "        if sb == 0:\n",
    "            #Set normal cell structure\n",
    "            self.econv1 = EqualizedLR_Conv2d(in_c, out_c, kernel_size=(3,3), stride=(1,1), padding=(1,1)) #Initial block a (alpha)\n",
    "            \n",
    "            self.econv2 = EqualizedLR_Conv2d(out_c, out_c, kernel_size=(4,4), stride=(1,1))\n",
    "            \n",
    "            self.outlayer = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        else:\n",
    "            self.mbstd = Minibatch_std()\n",
    "            self.econv2 = EqualizedLR_Conv2d(out_c, out_c, kernel_size=(4,4), stride=(1,1)) #output block\n",
    "            self.finish = nn.Sequential(nn.Flatten(), nn.Linear(out_c, out_c) ,nn.LeakyReLU(0.2, inplace=True), nn.Linear(out_c, 1))\n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### - Account for each discriminator block archetype - ###\n",
    "        if self.sb == 0:\n",
    "            x = self.econv1(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.econv(x)\n",
    "            x = self.relu(x)\n",
    "            \n",
    "            x = self.outlayer(x)\n",
    "        else:\n",
    "            x = self.mbstd(x)\n",
    "            x = self.econv2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.finish(x)\n",
    "        return x\n",
    "            \n",
    "#Generator Block\n",
    "\n",
    "class G_Cell(nn.Module):\n",
    "    def __init__(self, in_c, out_c, sb=0):\n",
    "        self.sb = sb\n",
    "        super().__init__()\n",
    "        \n",
    "        #Define network structure\n",
    "        if sb == 0:\n",
    "            self.us = nn.Upsample(scale_factor=2, mode='nearest') #Base block (standard cell)\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_c, out_c, kernel_size=(3,3), stride=(1,1), padding='same')\n",
    "            self.conv2 = EqualizedLR_Conv2d(in_c, out_c, kernel_size=(3,3), stride=(1,1), padding='same')\n",
    "        elif sb == 1:\n",
    "            self.dense = nn.Linear(in_c) #Our first initial training layer\n",
    "            self.conv1 = EqualizedLR_Conv2d(in_c, out_c, kernel_size=(3,3), stride=(1,1), padding='same')\n",
    "            \n",
    "        \n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.pn = Pixel_norm()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.sb == 0:\n",
    "            x = self.us(x)\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pn(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pn(x)\n",
    "        elif self.sb == 1:\n",
    "            x = self.pn(x)\n",
    "            x = self.dense(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pn(x)\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pn(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841ca2a-706f-4e35-baf0-81d62c76027b",
   "metadata": {},
   "source": [
    "# Network Structure and Basic Theory\n",
    "\n",
    "---\n",
    "\n",
    "Each network will progressively need to grow more and more in order to upscale the images while keeping each of the dims the same for upscaling it.\n",
    "Therefore to accurately train this model we construct it in such a way that we may output a 1 megapixel image.\n",
    "\n",
    "This involves defining the structure for the overall network once it is finished and including a depth index variable that will be increased in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "11d1f096-8a71-462e-b687-261d3f8d0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator and Generator#\n",
    "class G(nn.Module):\n",
    "    def __init__(self, ls, out):\n",
    "        \"\"\"\n",
    "        ls is latent size\n",
    "        out is desired output resolution\n",
    "        build structure iteratively\n",
    "        \"\"\"\n",
    "        super().__init()\n",
    "        self.depth = 1 #Current indexing\n",
    "        self.alpha = 1 #Fade value\n",
    "        self.incalpha = 0 #Value to increment alpha by\n",
    "        \n",
    "        self.trgb = tRGB(ls, 3) #torgb value\n",
    "        self.us = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.net = nn.ModuleList([G_Cell(ls, ls, sb=1)])\n",
    "        self.rgbs = nn.ModuleList([tRGB(latent_size, 3)])\n",
    "        \n",
    "        #Add all standard blocks\n",
    "        for i in range(2, int(np.log2(out))):\n",
    "            ### - trick is to decrease the latent vector as well for each of the higher level blocks - ###\n",
    "            if i < 7: \n",
    "                in_c = 1024\n",
    "                out_c = 1024\n",
    "            else:\n",
    "                in_c = int(1024 / 2**(i-7))\n",
    "                out_c = int(1024 / 2**(i-7))\n",
    "            self.net.append(G_Cell(in_c, out_c))\n",
    "            self.rgbs.append(tRGB(out_c, 3))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for cell in self.net[:self.depth-1]:\n",
    "            x = cell(x)\n",
    "        out = self.net[self.depth-1](x)\n",
    "        crgb = self.rgbs[self.depth-1](out)\n",
    "        if self.alpha < 1:\n",
    "            xprev = self.us(x)\n",
    "            rgbprev = self.rgbs[self.depth-2](xprev)\n",
    "            crgb = self.alpha * (rgbprev) + (1-self.alpha)*(crgb)\n",
    "        return crgb\n",
    "    def inc_depth(self, iters):\n",
    "        self.incalpha = 1/iters\n",
    "        self.alpha = 1/iters\n",
    "        self.depth += 1\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self,ls, out):\n",
    "        super().__init__()\n",
    "        self.depth = 1\n",
    "        self.alpha = 1\n",
    "        self.incalpha = 0\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.ds = nn.AvgPool2d(kernel_size(2,2), stride(2,2))\n",
    "        \n",
    "        self.net = nn.ModuleList([D_Cell(ls, ls, sb=3)]) #initialize final block\n",
    "        self.frgbs = nn.ModuleList([fRGB(3, ls)])\n",
    "        \n",
    "        for i in range(2, int(np.log2(out))):\n",
    "            if i < 7:\n",
    "                in_c, out_c = 1024, 1024\n",
    "            else:\n",
    "                in_c, out_c = int(512 / 2**(i - 5)), int(512 / 2**(i - 6))\n",
    "                \n",
    "            self.net.append(D_Cell(in_c, out_c))\n",
    "            self.frgbs.append(fRGB(3, in_c))\n",
    "            \n",
    "        def forward(self, x):\n",
    "            xc = self.frgbs[self.depth-1](x)\n",
    "            xc = self.net[self.depth-1](xc)\n",
    "            if self.alpha < 1: #if depth != 1\n",
    "                x = self.ds(x)\n",
    "                xprev = self.frgbs[self.depth-2](x)\n",
    "                xprev = self.relu(xprev)\n",
    "                xc = self.alpha*xprev + self.alpha*xc\n",
    "            for cell in reversed(self.net[:self.depth-1]):\n",
    "                xc = cell(xc)\n",
    "            \n",
    "            return xc\n",
    "        \n",
    "        def inc_depth(self, iters):\n",
    "            self.incalpha = 1/iters\n",
    "            self.alpha = 1/iters\n",
    "            self.depth += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fe87a-2d45-4fbe-a3a5-42076613f4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
